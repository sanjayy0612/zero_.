{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0OchTlMqtvfDMI+AY8HQy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjayy0612/zero_./blob/main/train/zero_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cUYJjhnokTd",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q transformers accelerate datasets peft bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"AnishJoshi/nl2bash-custom\", split=\"train\")\n",
        "dataset = dataset.train_test_split(test_size=0.2)  # Train/Test split\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "RUgFLeAvy2Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/codegemma-2b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/codegemma-2b\")\n",
        "\n",
        "# Example usage\n",
        "input_text = \"Write a Python function to reverse a string.\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i-YT8tihXCwI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "model_name = \"google/codegemma-2b\"\n",
        "\n",
        "# Quantization config (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "'''\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "'''"
      ],
      "metadata": {
        "id": "cooS2LErkc2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "def preprocess(example):\n",
        "\n",
        "    prompt = f\"Instruction: {example['nl_command']}\\nOutput: {example['bash_code']}{tokenizer.eos_token}\"\n",
        "\n",
        "\n",
        "    tokenized_prompt = tokenizer(prompt, truncation=True, max_length=192, padding=\"max_length\")\n",
        "\n",
        "\n",
        "    tokenized_prompt[\"labels\"] = tokenized_prompt[\"input_ids\"][:]\n",
        "\n",
        "    return tokenized_prompt\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, remove_columns=['nl_command', 'bash_code'])\n",
        "\n",
        "print(\"Preprocessing complete. Example of a tokenized sample:\")\n",
        "print(tokenized_dataset['train'][0].keys())"
      ],
      "metadata": {
        "id": "58ildUtOXxyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_part1 = tokenized_dataset[\"train\"].select(range(5000))\n",
        "train_dataset_part2 = tokenized_dataset[\"train\"].select(range(5000,10000))"
      ],
      "metadata": {
        "id": "sVtuXv25b4vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "\n",
        "training_args_part1 = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run1_5k_checkpoint\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer_part1 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_part1,\n",
        "    train_dataset=train_dataset_part1,\n",
        "\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"--- Starting Training: Part 1 (0 to 5,000 examples) ---\")\n",
        "trainer_part1.train()\n",
        "print(\"--- Part 1 Complete. Saving final model for this run. ---\")\n",
        "trainer_part1.save_model(\"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run1_5k_final\")"
      ],
      "metadata": {
        "id": "CMpzmwQ4mTA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Import all necessary libraries\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# STEP 2: Define the quantization configuration\n",
        "# This must be the same as what you used for training\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Path to your saved fine-tuned model\n",
        "model_path = \"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run2_10k_final\"\n",
        "\n",
        "print(\"\\nLoading model... (This may take a moment)\")\n",
        "# Reload base model + tokenizer\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/codegemma-2b\",\n",
        "    quantization_config=bnb_config, # Now bnb_config is defined\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/codegemma-2b\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load LoRA adapters on top of base model\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# CORRECTED Inference Function\n",
        "def generate_bash(nl_query):\n",
        "    prompt = f\"Instruction: {nl_query}\\nOutput:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.1,\n",
        "            do_sample=True\n",
        "        )\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    try:\n",
        "        bash_command = full_output.split(\"Output:\")[1].strip()\n",
        "    except IndexError:\n",
        "        bash_command = \"Error: Could not parse model output.\"\n",
        "    return bash_command\n",
        "\n",
        "# ---- Test Example ----\n",
        "test_query = \"Create a new directory called 'my_project'\"\n",
        "result = generate_bash(test_query)\n",
        "\n",
        "print(\"\\n--- INFERENCE RESULT ---\")\n",
        "print(\"Natural Language:\", test_query)\n",
        "print(\"---------------------------------\")\n",
        "print(\"Generated Bash Command:\", result)"
      ],
      "metadata": {
        "id": "1LezRldtt9l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT 5K data\n"
      ],
      "metadata": {
        "id": "DhS276iw68hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 1. Prepare the second data slice\n",
        "print(\"Preparing the next data slice (5,000 to 10,000)...\")\n",
        "train_dataset_part2 = tokenized_dataset[\"train\"].select(range(5000, 10000))\n",
        "print(f\"Data slice ready. Number of examples: {len(train_dataset_part2)}\")\n",
        "\n",
        "# 2. Define path to the model from Session 1\n",
        "model_path_run1 = \"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run1_5k_final\"\n",
        "\n",
        "# 3. Reload the base model\n",
        "print(f\"Loading base model 'google/codegemma-2b'...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/codegemma-2b\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/codegemma-2b\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 4. Load LoRA adapters with the FIX\n",
        "print(f\"Loading fine-tuned adapters from: {model_path_run1}\")\n",
        "model = PeftModel.from_pretrained(base_model, model_path_run1, is_trainable=True) # <-- THE FIX IS HERE\n",
        "print(\"Adapters from Part 1 loaded successfully.\")\n",
        "\n",
        "# Sanity Check: Print the number of trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 5. Set up training arguments\n",
        "training_args_part2 = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run2_10k_checkpoint\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 6. Create the Trainer\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "trainer_part2 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_part2,\n",
        "    train_dataset=train_dataset_part2,\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# 7. Start training!\n",
        "print(\"\\n--- Starting Training: Part 2 (5,000 to 10,000 examples) ---\")\n",
        "trainer_part2.train()\n",
        "\n",
        "# 8. Save the final model\n",
        "print(\"--- Part 2 Complete. Saving final model. ---\")\n",
        "trainer_part2.save_model(\"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run2_10k_final\")"
      ],
      "metadata": {
        "id": "nJxZNYNq7Avt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_prompts = [\n",
        "    \"Create a single directory named 'my_project'\",\n",
        "    \"Make just one folder called 'test_folder'\",\n",
        "    \"Create only the directory 'docs'\"\n",
        "]\n",
        "\n",
        "print(\"--- Testing Model with Engineered Prompts ---\")\n",
        "print(\"Model: Trained on 10k examples\")\n",
        "print(\"Temperature: 0.1\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Loop through each prompt and print the result\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"Test #{i+1}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "    # Generate the command using your function\n",
        "    generated_command = generate_bash(prompt)\n",
        "\n",
        "    print(f\"Result: {generated_command}\\n\")"
      ],
      "metadata": {
        "id": "F7JBFGsk-X61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,  # <-- This was the missing import\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "print(\"Preparing for the final training session...\")\n",
        "\n",
        "# Define the quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 1. Prepare the final data slice\n",
        "final_slice_start = 10000\n",
        "final_slice_end = len(tokenized_dataset[\"train\"])\n",
        "train_dataset_part3 = tokenized_dataset[\"train\"].select(range(final_slice_start, final_slice_end))\n",
        "print(f\"Data slice ready. Training on examples from {final_slice_start} to {final_slice_end}.\")\n",
        "\n",
        "# 2. Define path to the model from Session 2\n",
        "model_path_run2 = \"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run2_10k_final\"\n",
        "\n",
        "# 3. Reload the base model\n",
        "print(f\"Loading base model 'google/codegemma-b'...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/codegemma-2b\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/codegemma-2b\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 4. Load LoRA adapters from Session 2, making them trainable\n",
        "print(f\"Loading fine-tuned adapters from: {model_path_run2}\")\n",
        "model = PeftModel.from_pretrained(base_model, model_path_run2, is_trainable=True)\n",
        "print(\"Adapters from Part 2 loaded successfully.\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 5. Set up final training arguments\n",
        "training_args_part3 = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run3_final_checkpoint\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 6. Create the final Trainer\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "trainer_part3 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_part3,\n",
        "    train_dataset=train_dataset_part3,\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# 7. Start the final training run!\n",
        "print(\"\\n--- Starting Final Training: Part 3 ---\")\n",
        "trainer_part3.train()\n",
        "\n",
        "# 8. Save the final, fully-trained model\n",
        "print(\"--- Final Training Complete. Saving the final model. ---\")\n",
        "trainer_part3.save_model(\"/content/drive/MyDrive/codegemma_nl2bash_finetuned/final_model_15k\")"
      ],
      "metadata": {
        "id": "X5AXSDbyA4Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Testing the final model trained on all 15,726 examples...\")\n",
        "\n",
        "# Path to your FINAL, fully-trained model\n",
        "model_path = \"/content/drive/MyDrive/codegemma_nl2bash_finetuned/final_model_15k\"\n",
        "\n",
        "# Reload the model to ensure we're using the latest version\n",
        "print(\"Loading final model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"google/codegemma-2b\", quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/codegemma-2b\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "newline_token_id = tokenizer.encode('\\n', add_special_tokens=False)[0]\n",
        "\n",
        "# Using the perfected generate_bash function\n",
        "def generate_bash(nl_query):\n",
        "    prompt = f\"Instruction: {nl_query}\\nOutput:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "    inputs[\"input_ids\"].to(model.device),\n",
        "    max_new_tokens=50,       # prevent runaway sequences\n",
        "    temperature=0.0,         # greedy decoding\n",
        "    top_p=0.9,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    try:\n",
        "        bash_command = full_output.split(\"Output:\")[1].strip()\n",
        "    except IndexError:\n",
        "        bash_command = \"Error: Could not parse model output.\"\n",
        "    return bash_command\n",
        "\n",
        "# --- Test Cases ---\n",
        "prompts_to_test = [\n",
        "    # The original, ambiguous prompt that caused issues\n",
        "    \"Create a new directory called 'my_project'\",\n",
        "    # The specific, engineered prompt\n",
        "    \"Create a single directory named 'my_project'\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Running Final Evaluation ---\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for i, prompt in enumerate(prompts_to_test):\n",
        "    print(f\"Test #{i+1}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "    generated_command = generate_bash(prompt)\n",
        "\n",
        "    print(f\"Result: {generated_command}\\n\")"
      ],
      "metadata": {
        "id": "uXCxYu5_MkSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reduction of hallucination and overfitting retraining the 5k M\n"
      ],
      "metadata": {
        "id": "8MWTiT3oPQSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "print(\"Generating a high-quality correctional dataset of 500 examples...\")\n",
        "\n",
        "# A large and diverse pool of realistic directory names\n",
        "dir_names = [\n",
        "    \"src\", \"lib\", \"dist\", \"build\", \"assets\", \"components\", \"routes\", \"models\",\n",
        "    \"controllers\", \"services\", \"utils\", \"helpers\", \"middleware\", \"config\", \"public\",\n",
        "    \"static\", \"templates\", \"views\", \"tests\", \"docs\", \"data\", \"logs\", \"scripts\",\n",
        "    \"notebooks\", \"api\", \"_includes\", \".vscode\", \"migrations\", \"seeders\", \"database\",\n",
        "    \"core\", \"app\", \"main\", \"handlers\", \"modules\", \"plugins\", \"themes\", \"widgets\",\n",
        "    \"styles\", \"css\", \"js\", \"img\", \"fonts\", \"sass\", \"less\", \"dist-css\", \"build-js\",\n",
        "    \"server\", \"client\", \"shared\", \"common\", \"packages\", \"examples\", \"demo\", \"vendor\",\n",
        "    \"third_party\", \"bin\", \"etc\", \"var\", \"tmp\", \"temp\", \"backups\", \"uploads\", \"media\",\n",
        "    \"screenshots\", \"videos\", \"audio\", \"content\", \"pages\", \"posts\", \"layouts\",\n",
        "    \"production\", \"development\", \"staging\", \"dockerfiles\", \"kubernetes\",\n",
        "    \"_data\", \"functions\", \"lambda\", \"hooks\", \"providers\", \"contexts\", \"reducers\",\n",
        "    \"actions\", \"store\", \"features\", \"epics\", \"sagas\", \"docker\", \".github\", \"ci\",\n",
        "    \"workflows\", \"dev-docs\", \"user_guides\", \"marketing-assets\", \"legal\", \"archive\",\n",
        "    \"dist-v1\", \"release-build\", \"final-assets\", \"go-project\", \"python_app\",\n",
        "    \"node_server\", \"react-client\", \"infra\", \"terraform\", \"ansible\", \"images_final\"\n",
        "]\n",
        "\n",
        "# A variety of templates for natural language commands\n",
        "prompt_templates = [\n",
        "    lambda d: f\"create a single directory named '{d}'\",\n",
        "    lambda d: f\"make the folder '{d}'\",\n",
        "    lambda d: f\"make a directory called '{d}'\",\n",
        "    lambda d: f\"create a folder for {d}\",\n",
        "    lambda d: f\"new folder: '{d}'\",\n",
        "    lambda d: f\"make just one directory called '{d}'\",\n",
        "    lambda d: f\"create the '{d}' directory\",\n",
        "    lambda d: f\"form a directory, name it '{d}'\",\n",
        "    lambda d: f\"generate a directory called '{d}'\",\n",
        "    lambda d: f\"please make a folder called '{d}'\",\n",
        "    lambda d: f\"I need a directory, call it '{d}'\",\n",
        "    lambda d: f\"folder '{d}'\",\n",
        "    lambda d: f\"directory '{d}'\",\n",
        "    lambda d: f\"make a dir '{d}'\",\n",
        "    lambda d: f\"create dir '{d}'\",\n",
        "    lambda d: f\"I want a new folder, name it '{d}'\",\n",
        "    lambda d: f\"can you create the '{d}' directory?\",\n",
        "    lambda d: f\"just the directory '{d}', please\",\n",
        "    lambda d: f\"only create the folder '{d}'\"\n",
        "]\n",
        "\n",
        "correctional_data = []\n",
        "used_combinations = set()\n",
        "\n",
        "# Generate 500 unique examples\n",
        "while len(correctional_data) < 500:\n",
        "    dir_name = random.choice(dir_names)\n",
        "    template = random.choice(prompt_templates)\n",
        "\n",
        "    nl_command = template(dir_name)\n",
        "    bash_command = f\"mkdir {dir_name}\"\n",
        "\n",
        "    # Ensure we don't have duplicate entries\n",
        "    if (nl_command, bash_command) not in used_combinations:\n",
        "        correctional_data.append({\"nl_command\": nl_command, \"bash_code\": bash_command})\n",
        "        used_combinations.add((nl_command, bash_command))\n",
        "\n",
        "# The path to save the new file in your Google Drive\n",
        "file_path = \"/content/drive/MyDrive/correctional_dataset_500.jsonl\"\n",
        "\n",
        "# Write the data to a .jsonl file\n",
        "with open(file_path, 'w') as f:\n",
        "    for entry in correctional_data:\n",
        "        f.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"Successfully created correctional dataset at: {file_path}\")\n",
        "print(f\"It contains {len(correctional_data)} high-quality, unique examples.\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Example #1:\", correctional_data[0])\n",
        "print(\"Example #2:\", correctional_data[1])"
      ],
      "metadata": {
        "id": "2eSy9zkgOSfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(correctional_data))"
      ],
      "metadata": {
        "id": "gNeNGLXfSWwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "print(\"Starting the correctional fine-tuning process...\")\n",
        "\n",
        "# --- 1. Load the new, high-quality dataset ---\n",
        "correctional_dataset_path = \"/content/drive/MyDrive/correctional_dataset_500.jsonl\"\n",
        "print(f\"Loading correctional dataset from: {correctional_dataset_path}\")\n",
        "dataset = load_dataset(\"json\", data_files=correctional_dataset_path, split=\"train\")\n",
        "\n",
        "# Split the small dataset into training and testing sets (90% train, 10% test)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "print(\"Dataset loaded and split:\", dataset)\n",
        "\n",
        "\n",
        "# --- 2. Preprocess the dataset ---\n",
        "def preprocess(example):\n",
        "    prompt = f\"Instruction: {example['nl_command']}\\nOutput: {example['bash_code']}\"\n",
        "    return tokenizer(prompt, truncation=True, max_length=128)\n",
        "\n",
        "\n",
        "# --- 3. Load our starting model (the 5k version) ---\n",
        "# Define quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Path to our best model from Session 1\n",
        "model_path_run1 = \"/content/drive/MyDrive/codegemma_nl2bash_finetuned/run1_5k_final\"\n",
        "\n",
        "print(f\"Loading base model and adapters from: {model_path_run1}\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/codegemma-2b\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/codegemma-2b\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Load the adapters and make them trainable\n",
        "model = PeftModel.from_pretrained(base_model, model_path_run1, is_trainable=True)\n",
        "print(\"Base model and 5k adapters loaded successfully.\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Apply the preprocessing to the new dataset\n",
        "tokenized_dataset = dataset.map(preprocess, remove_columns=['nl_command', 'bash_code'])\n",
        "\n",
        "\n",
        "# --- 4. Define specialized Training Arguments ---\n",
        "# We use a lower learning rate and fewer epochs for this delicate operation\n",
        "correctional_training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/codegemma_surgically_corrected_checkpoint\",\n",
        "    num_train_epochs=2,         # Only 2 epochs is enough for this small dataset\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,         # Lower learning rate for fine adjustments\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "# --- 5. Run the Correctional Training ---\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=correctional_training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting the Surgical Strike Fine-Tuning ---\")\n",
        "trainer.train()\n",
        "\n",
        "# --- 6. Save the final, corrected model ---\n",
        "print(\"--- Correctional Training Complete. Saving the final corrected model. ---\")\n",
        "trainer.save_model(\"/content/drive/MyDrive/codegemma_surgically_corrected_model\")\n",
        "\n",
        "print(\"\\nProcess complete! The new model should have its 'mkdir' behavior corrected.\")"
      ],
      "metadata": {
        "id": "0QqP5aupOTw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Testing the final, surgically corrected model...\")\n",
        "\n",
        "# Path to the final, corrected model\n",
        "model_path = \"/content/drive/MyDrive/codegemma_surgically_corrected_model\"\n",
        "\n",
        "# Reload the model\n",
        "print(\"Loading final model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"google/codegemma-2b\", quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/codegemma-2b\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Using the perfected generate_bash function\n",
        "newline_token_id = tokenizer.encode('\\n', add_special_tokens=False)[0]\n",
        "def generate_bash(nl_query):\n",
        "    prompt = f\"Instruction: {nl_query}\\nOutput:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            eos_token_id=newline_token_id\n",
        "        )\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    try:\n",
        "        bash_command = full_output.split(\"Output:\")[1].strip()\n",
        "    except IndexError:\n",
        "        bash_command = \"Error: Could not parse model output.\"\n",
        "    return bash_command\n",
        "\n",
        "# --- Test Cases ---\n",
        "prompts_to_test = [\n",
        "    # Test 1: The original ambiguous prompt. Will it be literal now?\n",
        "    \"Create a new directory called 'my_project'\",\n",
        "    # Test 2: The specific prompt. This MUST work.\n",
        "    \"Create a single directory named 'my_project_2'\",\n",
        "    # Test 3: A different command to check for damage.\n",
        "    \"list all files and folders in the current directory in a long format\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Running Final Evaluation ---\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for i, prompt in enumerate(prompts_to_test):\n",
        "    print(f\"Test #{i+1}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    generated_command = generate_bash(prompt)\n",
        "    print(f\"Result: '{generated_command}'\\n\")"
      ],
      "metadata": {
        "id": "RtTiznDjSbW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging the lora file with modelfiles\n"
      ],
      "metadata": {
        "id": "RF5JO1n-suEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Starting model merge and upload process...\")\n",
        "\n",
        "# --- 1. Load the Base Model and the Fine-Tuned Adapters ---\n",
        "\n",
        "# Path to your final, surgically corrected model\n",
        "adapter_path = \"/content/drive/MyDrive/codegemma_surgically_corrected_model\"\n",
        "base_model_name = \"google/codegemma-2b\"\n",
        "\n",
        "# Load the base model in full precision (bfloat16) for merging\n",
        "print(\"Loading base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading your fine-tuned adapters...\")\n",
        "# Load the LoRA model\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "\n",
        "# --- 2. Merge the Adapters into the Base Model ---\n",
        "\n",
        "print(\"Merging adapters into the base model...\")\n",
        "model = model.merge_and_unload()\n",
        "print(\"Merge complete.\")\n",
        "\n",
        "\n",
        "# --- 3. Log in to Hugging Face Hub ---\n",
        "\n",
        "# Make sure your HF_TOKEN with 'write' permission is in your Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if not hf_token:\n",
        "    print(\"❌ HF_TOKEN not found in Colab secrets. Please add it before running.\")\n",
        "else:\n",
        "    login(token=hf_token)\n",
        "    print(\"✅ Successfully logged into Hugging Face Hub.\")\n",
        "\n",
        "\n",
        "# --- 4. Push the Merged Model and Tokenizer to the Hub ---\n",
        "\n",
        "# This has been updated with your new repository name from the screenshot\n",
        "new_repo_name = \"Sanjayyy06/zero-nl2cmds-v1\"\n",
        "\n",
        "print(f\"\\nPushing merged model to Hub repository: {new_repo_name}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "try:\n",
        "    # Push the model\n",
        "    model.push_to_hub(\n",
        "        new_repo_name,\n",
        "        commit_message=\"Initial commit of surgically corrected model v1\",\n",
        "        private=False\n",
        "    )\n",
        "\n",
        "    # Push the tokenizer\n",
        "    tokenizer.push_to_hub(\n",
        "        new_repo_name,\n",
        "        commit_message=\"Add tokenizer\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅✅✅ Successfully pushed model and tokenizer to the Hugging Face Hub!\")\n",
        "    print(f\"You can find your model at: https://huggingface.co/{new_repo_name}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An error occurred during the push: {e}\")"
      ],
      "metadata": {
        "id": "hNufzlh6q8K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing the merges verision model\n"
      ],
      "metadata": {
        "id": "wgbjrrJzSDar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "print(\"Testing the final merged model from the Hugging Face Hub...\")\n",
        "\n",
        "# --- 1. Define Model and Quantization Config ---\n",
        "\n",
        "# The Hugging Face Hub repository ID for your merged model\n",
        "model_id = \"Sanjayyy06/zero-nl2cmds-v1\"\n",
        "\n",
        "# Set up 4-bit quantization for efficient inference in Colab\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# --- 2. Load Model and Tokenizer from the Hub ---\n",
        "\n",
        "print(f\"Loading model: {model_id}\")\n",
        "# This is now a standard, one-step loading process\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "\n",
        "# --- 3. The Inference Function ---\n",
        "# (Same perfected function as before)\n",
        "newline_token_id = tokenizer.encode('\\n', add_special_tokens=False)[0]\n",
        "\n",
        "def generate_bash(nl_query):\n",
        "    prompt = f\"Instruction: {nl_query}\\nOutput:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            eos_token_id=newline_token_id\n",
        "        )\n",
        "\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    try:\n",
        "        bash_command = full_output.split(\"Output:\")[1].strip()\n",
        "    except IndexError:\n",
        "        bash_command = \"Error: Could not parse model output.\"\n",
        "\n",
        "    return bash_command\n",
        "\n",
        "\n",
        "# --- 4. Run the Final Test Cases ---\n",
        "prompts_to_test = [\n",
        "    # The ambiguous prompt that previously failed\n",
        "    \"Create a new directory called 'my_project'\",\n",
        "    # The specific prompt\n",
        "    \"Create a single directory named 'my_project_2'\",\n",
        "    # The general knowledge check\n",
        "    \"list all files and folders in the current directory in a long format\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Running Final Evaluation ---\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for i, prompt in enumerate(prompts_to_test):\n",
        "    print(f\"Test #{i+1}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    generated_command = generate_bash(prompt)\n",
        "    print(f\"Result: '{generated_command}'\\n\")"
      ],
      "metadata": {
        "id": "btGUuaBjSIiM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Setup and Build\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Clean Slate & Install Packages ---\n",
        "print(\"\\nCleaning up previous runs and installing packages...\")\n",
        "!rm -rf /content/llama.cpp\n",
        "!rm -rf /content/hub_model\n",
        "!pip install llama-cpp-python huggingface_hub sentencepiece protobuf mistral_common -q\n",
        "\n",
        "# --- Clone llama.cpp Repo ---\n",
        "print(\"\\nCloning the llama.cpp repository...\")\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git /content/llama.cpp\n",
        "\n",
        "# --- Build the Tools with CMake ---\n",
        "print(\"\\nBuilding the conversion tools (this will take a few minutes)...\")\n",
        "!cd /content/llama.cpp && mkdir -p build && cd build && cmake .. && cmake --build . --config Release\n",
        "\n",
        "print(\"\\n✅ Block 1 Complete: Environment is ready.\")"
      ],
      "metadata": {
        "id": "s7dpSa1MAFbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Download & Convert (Robust Version)\n",
        "\n",
        "import os\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# --- Download Your Model ---\n",
        "model_id = \"Sanjayyy06/zero-nl2cmds-v1\"\n",
        "model_dir = \"/content/hub_model\"\n",
        "print(f\"Downloading model '{model_id}' from the Hub...\")\n",
        "snapshot_download(repo_id=model_id, local_dir=model_dir, local_dir_use_symlinks=False)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "# --- Dynamically Find and Run the Conversion Script ---\n",
        "# **THIS IS THE CRITICAL FIX**\n",
        "fp16_gguf_path = \"/content/model-f16.gguf\"\n",
        "print(\"\\nFinding and running the correct conversion script...\")\n",
        "\n",
        "# This shell command block finds the script, checks if it exists, and then runs it.\n",
        "convert_command = f\"\"\"\n",
        "set -e\n",
        "CONVERT_SCRIPT_PATH=$(find /content/llama.cpp -name \"*convert*.py\" | head -n 1)\n",
        "\n",
        "if [ -z \"$CONVERT_SCRIPT_PATH\" ]; then\n",
        "    echo \"❌ CRITICAL ERROR: Could not find the model conversion python script in the llama.cpp repository.\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "echo \"✅ Found conversion script at: $CONVERT_SCRIPT_PATH\"\n",
        "python3 \"$CONVERT_SCRIPT_PATH\" {model_dir} --outfile {fp16_gguf_path} --outtype f16\n",
        "\"\"\"\n",
        "get_ipython().system(convert_command)\n",
        "\n",
        "\n",
        "# --- Check if conversion was successful before proceeding ---\n",
        "if not os.path.exists(fp16_gguf_path):\n",
        "    print(\"\\n❌ ERROR: Conversion to FP16 GGUF failed. The intermediate file was not created. Please check the logs above.\")\n",
        "else:\n",
        "    print(\"Conversion to FP16 GGUF successful.\")\n",
        "    # --- Quantize to 4-bit GGUF ---\n",
        "    final_gguf_name = \"zero-nl2cmds-v1.Q4_K_M.gguf\"\n",
        "    final_gguf_path_colab = f\"/content/{final_gguf_name}\"\n",
        "    print(f\"\\nQuantizing to final GGUF file: {final_gguf_name}...\")\n",
        "    !/content/llama.cpp/build/bin/llama-quantize {fp16_gguf_path} {final_gguf_path_colab} Q4_K_M\n",
        "\n",
        "    print(\"\\n✅ Block 2 Complete: The final GGUF file has been created in the Colab environment.\")\n",
        "    print(\"You can see it listed below:\")\n",
        "    !ls -lh {final_gguf_path_colab}"
      ],
      "metadata": {
        "id": "kbUOYbEFCkBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Move to Google Drive and Clean Up\n",
        "\n",
        "import os\n",
        "\n",
        "# --- Define Paths ---\n",
        "final_gguf_name = \"zero-nl2cmds-v1.Q4_K_M.gguf\"\n",
        "final_gguf_path_colab = f\"/content/{final_gguf_name}\"\n",
        "gdrive_output_dir = \"/content/drive/MyDrive/GGUF_Models/\"\n",
        "os.makedirs(gdrive_output_dir, exist_ok=True)\n",
        "final_gguf_path_gdrive = os.path.join(gdrive_output_dir, final_gguf_name)\n",
        "\n",
        "# --- Check for the file before copying ---\n",
        "if not os.path.exists(final_gguf_path_colab):\n",
        "    print(f\"❌ ERROR: Cannot find the file '{final_gguf_name}' to copy. Did Block 2 run successfully?\")\n",
        "else:\n",
        "    # --- Copy to Google Drive ---\n",
        "    print(f\"Moving '{final_gguf_name}' to your Google Drive...\")\n",
        "    !cp {final_gguf_path_colab} \"{final_gguf_path_gdrive}\"\n",
        "    print(\"Move complete.\")\n",
        "\n",
        "    # --- Clean Up Colab Environment ---\n",
        "    print(\"\\nCleaning up temporary files...\")\n",
        "    # Using -f to force removal and ignore \"not found\" errors from previous runs\n",
        "    !rm -f /content/model-f16.gguf\n",
        "    !rm -f {final_gguf_path_colab}\n",
        "    !rm -rf /content/hub_model\n",
        "    !rm -rf /content/llama.cpp\n",
        "    print(\"Cleanup complete.\")\n",
        "\n",
        "    # --- Final Confirmation ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅✅✅ PROCESS COMPLETE! ✅✅✅\")\n",
        "    print(\"Your final model file is now permanently saved in your Google Drive.\")\n",
        "    print(\"You can find it in the 'GGUF_Models' folder.\")\n",
        "    print(\"Here are the details:\")\n",
        "    !ls -lh \"{final_gguf_path_gdrive}\"\n",
        "    print(\"\\nWe are now ready to build the local CLI tool.\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "beQSXDyQEqF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}